# Text and Image Similarity Score

Various scripts that take as input an image and a string of text and return a 
score for how similar the two are. 

### Getting Started

The easiest way to run the code is to use [NVIDIA Docker](https://github.com/NVIDIA/nvidia-docker).

On Ubuntu 18.04, once you have Docker installed you can install NVIDIA Docker 
with:

```shell script
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
``` 

### Prerequisites

Training the models on a GPU is highly recommended. A GPU with at least 10GB of 
VRAM will be required.

### Installing

First, set up the Docker container using:

```shell script
sh scripts/run-in-docker.sh
```

This will download the `tensorflow:nightly-gpu-py3` Docker image from Docker 
Hub and install all pre-requisites.

### Training the models

Once inside the Docker container, you will first need to train the image 
captioning model. This is done by running:

```shell script
python train.py
```

Note that this may take a few weeks to finish, depending on how powerful your 
system is. An 8 x NVIDIA Tesla V100 system should be able to complete the 
training in under a day. The code supports multi-GPU training, so if your system has multiple devices, they will all be used unless specified by setting the `CUDA_VISIBLE_DEVICES` flag.

Once training is complete, the model architecture (a JSON file generated by 
Keras) and the model weights (a .h5 file) will be saved in the `checkpoints` 
folder.

### Inference

Once the models are trained and checkpoints are saved, to generate a similarity 
score between an image and some text, run the following command with the image 
saved locally on device:

```shell script
python get_similarity_score.py --image_path <path_to_image> --text "Some text"
```

Ideally, this should be deployed using TensorFlow Serving, where a production
server already has the model loaded into memory and is waiting for API
requests to be made with images and strings of text. That way, we don't
have to re-load the model every time we want to predict on a new batch of
data, which can be very time consuming.